{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riccardocappi/Machine-Learning-From-Scratch/blob/main/Back_Propagation_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Back-propagation from scratch###\n",
        "Implementation from scratch of Backpropagation. The algorithm is tested to train a 2-layer neural network with a sigmoid activation function for the hidden layer, and a softmax output layer. The network learns an encoding of the identity function."
      ],
      "metadata": {
        "id": "kKJw5Rhwhd-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "ceJTHqIpJUPW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Network components###\n"
      ],
      "metadata": {
        "id": "X6n0bP6FlO5d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "piK-X0fCo0KK"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def softmax(x):\n",
        "  ex = np.exp(x)\n",
        "  return ex / ex.sum()\n",
        "\n",
        "\n",
        "\n",
        "class HiddenLayer:\n",
        "  def __init__(self, input_size, dim, is_last_layer):\n",
        "    self.W = np.random.uniform(size=(input_size, dim), high=0.05, low=-0.05)\n",
        "    self.b = np.random.uniform(size=dim, high=0.05, low=-0.05)\n",
        "    self.af = softmax if is_last_layer else sigmoid\n",
        "    self.h = None\n",
        "    self.is_last_layer = is_last_layer\n",
        "    self.last_input = None\n",
        "    self.Delta_W = np.zeros_like(self.W)\n",
        "    self.Delta_b = np.zeros_like(self.b)\n",
        "\n",
        "  def compute_neurons_activations(self, input):\n",
        "    h = self.af(np.dot(input, self.W) + self.b)\n",
        "    self.last_input = input\n",
        "    self.h = h\n",
        "    return h\n",
        "\n",
        "  def compute_gradients(self, t_i, return_delta):\n",
        "    if self.is_last_layer:\n",
        "      error = self.h - t_i\n",
        "    else:\n",
        "      error = self.h * (1 - self.h) * t_i\n",
        "    self.Delta_W += np.outer(self.last_input, error)\n",
        "    self.Delta_b += error\n",
        "    return (self.W * error).sum(axis=1) if return_delta else None\n",
        "\n",
        "  def update_weights(self, lr):\n",
        "    self.W += (-lr * self.Delta_W)\n",
        "    self.b += (-lr * self.Delta_b)\n",
        "    self.Delta_W.fill(0)\n",
        "    self.Delta_b.fill(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(network, X, y, lr, steps):\n",
        "  S = list(zip(X,y))\n",
        "  print('Learning started')\n",
        "  for _ in range(steps):\n",
        "    for x,t in S:\n",
        "      #Forward phase\n",
        "      h = x\n",
        "      for hl in network:\n",
        "        h = hl.compute_neurons_activations(h)\n",
        "      #Backpropagation\n",
        "      delta = network[-1].compute_gradients(t, True)\n",
        "      for i in reversed(range(len(network)-1)):\n",
        "        hl = network[i]\n",
        "        delta = hl.compute_gradients(delta, i != 0)\n",
        "      #Update weights\n",
        "      for hl in network:\n",
        "        hl.update_weights(lr)\n",
        "  return network\n"
      ],
      "metadata": {
        "id": "1qrFfTVoBgnY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "X = np.identity(8)\n",
        "y = np.identity(8)\n",
        "\n",
        "h1 = HiddenLayer(8,3, False)\n",
        "out = HiddenLayer(3, 8, True)\n",
        "network = [h1,out]\n",
        "\n",
        "network = train(network,X,y,0.2,6000)\n",
        "print('Input\\t\\t\\t\\t\\t', 'Hidden Values\\t\\t\\t\\t','Output')\n",
        "for x in X:\n",
        "  h = x\n",
        "  activations = []\n",
        "  for hl in network:\n",
        "    h = hl.compute_neurons_activations(h)\n",
        "    activations.append(h)\n",
        "  y = activations[0]\n",
        "  z = activations[-1]\n",
        "  print(x, '\\t\\t'+ str([round(y_i,2) for y_i in y])+'\\t\\t\\t', [0 if z_i <0.5 else 1 for z_i in z])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afT9BdC6Vn5p",
        "outputId": "09b25b94-fee3-4900-cd7e-cb3da54705c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning started\n",
            "Input\t\t\t\t\t Hidden Values\t\t\t\t Output\n",
            "[1. 0. 0. 0. 0. 0. 0. 0.] \t\t[0.15, 0.99, 0.99]\t\t\t [1, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0. 1. 0. 0. 0. 0. 0. 0.] \t\t[0.98, 0.02, 0.01]\t\t\t [0, 1, 0, 0, 0, 0, 0, 0]\n",
            "[0. 0. 1. 0. 0. 0. 0. 0.] \t\t[0.0, 0.98, 0.06]\t\t\t [0, 0, 1, 0, 0, 0, 0, 0]\n",
            "[0. 0. 0. 1. 0. 0. 0. 0.] \t\t[0.99, 0.81, 0.89]\t\t\t [0, 0, 0, 1, 0, 0, 0, 0]\n",
            "[0. 0. 0. 0. 1. 0. 0. 0.] \t\t[0.03, 0.02, 0.01]\t\t\t [0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0. 0. 0. 0. 0. 1. 0. 0.] \t\t[0.01, 0.11, 0.97]\t\t\t [0, 0, 0, 0, 0, 1, 0, 0]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0.] \t\t[0.9, 0.99, 0.01]\t\t\t [0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[0. 0. 0. 0. 0. 0. 0. 1.] \t\t[0.89, 0.01, 0.99]\t\t\t [0, 0, 0, 0, 0, 0, 0, 1]\n"
          ]
        }
      ]
    }
  ]
}